{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f319e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 15:08:57.574084: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-10 15:08:57.601945: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-10 15:08:58.210213: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "# Added Label smoothing & AdamW and SE blocks and Slightly stronger SpecAugment\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as L, models as M, callbacks as C\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f312553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "#           PATHS\n",
    "# =========================\n",
    "DRONE_ROOT   = r\"/home/destrox-907/Husnian_FYP/Dataset/MFCC Drone Dataset/MFCC Drone Dataset\"\n",
    "NODRONE_ROOT = r\"/home/destrox-907/Husnian_FYP/Dataset/MFCC Noise Dataset/MFCC Noise Dataset\"\n",
    "\n",
    "# =========================\n",
    "#      CONFIG / HYPERS\n",
    "# =========================\n",
    "INPUT_SHAPE     = (13, 40, 1)\n",
    "BATCH_SIZE      = 128\n",
    "EPOCHS          = 40\n",
    "LEARNING_RATE   = 1e-3\n",
    "MIXUP_ALPHA     = 0.3\n",
    "USE_SPECAUG     = True\n",
    "VAL_SIZE        = 0.20\n",
    "SEED            = 42\n",
    "\n",
    "# =========================\n",
    "#  RECURSIVE DATA LOADING\n",
    "# =========================\n",
    "def collect_npy_paths(root_dir):\n",
    "    paths = []\n",
    "    for r, _, files in os.walk(root_dir):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(\".npy\"):\n",
    "                paths.append(os.path.join(r, f))\n",
    "    return paths\n",
    "\n",
    "def load_paths_and_labels(drone_root, nodrone_root):\n",
    "    drone_paths   = collect_npy_paths(drone_root)\n",
    "    nodrone_paths = collect_npy_paths(nodrone_root)\n",
    "    paths  = np.array(drone_paths + nodrone_paths)\n",
    "    labels = np.array([1]*len(drone_paths) + [0]*len(nodrone_paths), dtype=np.int32)\n",
    "    return paths, labels\n",
    "\n",
    "paths, labels = load_paths_and_labels(DRONE_ROOT, NODRONE_ROOT)\n",
    "print(f\"Total .npy files: {len(paths)} | drone={labels.sum()} | no_drone={len(labels)-labels.sum()}\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    paths, labels, test_size=VAL_SIZE, random_state=SEED, stratify=labels\n",
    ")\n",
    "\n",
    "# =========================\n",
    "#     DATA PIPELINE\n",
    "# =========================\n",
    "def npy_loader(path):\n",
    "    try:\n",
    "        arr = np.load(path.decode(\"utf-8\")).astype(\"float32\")\n",
    "        arr = np.reshape(arr, (13, 40, 1))\n",
    "        return arr\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to load {path.decode('utf-8')}: {e}\")\n",
    "        return np.zeros((13, 40, 1), dtype=\"float32\")\n",
    "\n",
    "def tf_load(path, label, training=True):\n",
    "    x = tf.numpy_function(npy_loader, [path], Tout=tf.float32)\n",
    "    x = tf.ensure_shape(x, INPUT_SHAPE)\n",
    "\n",
    "    if training and USE_SPECAUG:\n",
    "        # >>> CHANGED: stronger SpecAugment\n",
    "        def _specaug(a):\n",
    "            a = a.copy()\n",
    "            f_dim, t_dim, _ = a.shape  # 13,40,1\n",
    "            for _ in range(3):\n",
    "                f = np.random.randint(0, 5)      # up to 4 MFCC bins\n",
    "                if f > 0:\n",
    "                    f0 = np.random.randint(0, f_dim - f + 1)\n",
    "                    a[f0:f0+f, :, :] = 0.0\n",
    "            for _ in range(3):\n",
    "                t = np.random.randint(0, 11)     # up to 10 time frames\n",
    "                if t > 0:\n",
    "                    t0 = np.random.randint(0, t_dim - t + 1)\n",
    "                    a[:, t0:t0+t, :] = 0.0\n",
    "            return a\n",
    "        x = tf.numpy_function(_specaug, [x], Tout=tf.float32)\n",
    "        x = tf.ensure_shape(x, INPUT_SHAPE)\n",
    "\n",
    "    y = tf.one_hot(label, 2)\n",
    "    return x, y\n",
    "\n",
    "def make_dataset(paths, labels, batch=BATCH_SIZE, training=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    if training:\n",
    "        ds = ds.shuffle(8192, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(lambda p, l: tf_load(p, l, training=training),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch)\n",
    "\n",
    "    if training and MIXUP_ALPHA > 0.0:\n",
    "        def mixup_batch(x, y):\n",
    "            lam = tf.random.uniform([], 0.5, 1.0)\n",
    "            idx = tf.random.shuffle(tf.range(tf.shape(x)[0]))\n",
    "            x2 = tf.gather(x, idx); y2 = tf.gather(y, idx)\n",
    "            return lam*x + (1-lam)*x2, lam*y + (1-lam)*y2\n",
    "        ds = ds.map(mixup_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = make_dataset(X_train, y_train, training=True)\n",
    "val_ds   = make_dataset(X_val,   y_val,   training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f61864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "#   RESNET-LIKE BACKBONE (+SE)\n",
    "# =========================\n",
    "def squeeze_excite(x, r=8):\n",
    "    c = x.shape[-1]\n",
    "    s = L.GlobalAveragePooling2D()(x)\n",
    "    s = L.Dense(max(c//r, 8), activation=\"relu\")(s)\n",
    "    s = L.Dense(c, activation=\"sigmoid\")(s)\n",
    "    s = L.Reshape((1,1,c))(s)\n",
    "    return L.Multiply()([x, s])\n",
    "\n",
    "def res_block(x, filters, stride=1, use_se=False):\n",
    "    shortcut = x\n",
    "    x = L.Conv2D(filters, (3,3), strides=stride, padding=\"same\", use_bias=False)(x)\n",
    "    x = L.BatchNormalization()(x); x = L.ReLU()(x)\n",
    "    x = L.Conv2D(filters, (3,3), padding=\"same\", use_bias=False)(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    if use_se:\n",
    "        x = squeeze_excite(x)\n",
    "    if shortcut.shape[-1] != filters or stride != 1:\n",
    "        shortcut = L.Conv2D(filters, (1,1), strides=stride, padding=\"same\", use_bias=False)(shortcut)\n",
    "        shortcut = L.BatchNormalization()(shortcut)\n",
    "    x = L.Add()([x, shortcut]); x = L.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "def build_model(input_shape=(13,40,1), num_classes=2):\n",
    "    inp = L.Input(shape=input_shape)\n",
    "    x = L.Conv2D(32, (3,3), padding=\"same\", use_bias=False)(inp)\n",
    "    x = L.BatchNormalization()(x); x = L.ReLU()(x)\n",
    "\n",
    "    x = res_block(x, 32, use_se=True)\n",
    "    x = res_block(x, 32, use_se=True)\n",
    "    x = L.MaxPool2D(pool_size=(1,2))(x)\n",
    "\n",
    "    x = res_block(x, 64, stride=2, use_se=True)\n",
    "    x = res_block(x, 64, use_se=True)\n",
    "\n",
    "    x = L.GlobalAveragePooling2D()(x)\n",
    "    x = L.Dense(128, activation=\"relu\")(x)\n",
    "    x = L.Dropout(0.3)(x)\n",
    "    out = L.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return M.Model(inp, out)\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "#      TRAIN & EVAL\n",
    "# =========================\n",
    "# >>> CHANGED: Label smoothing + AdamW\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05)\n",
    "opt  = tf.keras.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=1e-4)\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "cbs = [\n",
    "    C.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.5, patience=3, verbose=1),\n",
    "    C.EarlyStopping(monitor=\"val_accuracy\", patience=8, restore_best_weights=True),\n",
    "    C.ModelCheckpoint(\"drone_resnet_best.keras\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=cbs)\n",
    "\n",
    "probs = model.predict(val_ds)[:, 1]\n",
    "y_true = np.concatenate([y.numpy() for _, y in val_ds])[:, 1]\n",
    "auroc = roc_auc_score(y_true, probs)\n",
    "prec, rec, thr = precision_recall_curve(y_true, probs)\n",
    "auprc = auc(rec, prec)\n",
    "cands = np.linspace(0.05, 0.95, 19)\n",
    "f1s = [f1_score(y_true, (probs >= t).astype(int)) for t in cands]\n",
    "best_thr, best_f1 = cands[int(np.argmax(f1s))], max(f1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bcbb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SAVE THE FINAL MODEL (whatever weights are in memory now) ---\n",
    "model.save(\"drone_resnet_final_form.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a19fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n=== Validation Metrics ===\")\n",
    "print(f\"AUROC  : {auroc:.4f}\")\n",
    "print(f\"AUPRC  : {auprc:.4f}\")\n",
    "print(f\"Best F1: {best_f1:.4f} @ threshold={best_thr:.2f}\")\n",
    "\n",
    "y_pred = (probs >= best_thr).astype(int)\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
    "print(cm)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Drone\", \"Drone\"])\n",
    "disp.plot(cmap=plt.cm.Blues, values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"No Drone\", \"Drone\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
